{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from evaluation import get_mean_cov_for_each_label, frechet_distance\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real disbrituion for each label.\n",
    "# Image data: values are transformed to [0, 1],\n",
    "#             same as that of generator outputs.\n",
    "# Dictionary stats_real, example: stats[0] = [mean_0, cov_0]\n",
    "stats_real = get_mean_cov_for_each_label(datasets='mnist')\n",
    "\n",
    "\n",
    "# Calculate FID for each label.\n",
    "# 1. For each label i,\n",
    "#    use 1000 fake images to calculate the generated distribution.\n",
    "# 2. Then calculate fid for each label using fake & real distribution.\n",
    "nClass = 10\n",
    "fake_num = 6400\n",
    "stats_fake = {}\n",
    "\n",
    "trained_generator_mnist = model_mnist.g # mnist generator\n",
    "latent_points, _ = generate_latent_points(model_mnist.z_dim, fake_num, 'uniform')\n",
    "\n",
    "for i in range(nClass):\n",
    "    labels = np.full(shape=(fake_num,), fill_value=i, dtype=np.int32)\n",
    "    labels = tf.one_hot(labels, 10).numpy()\n",
    "    # Get input to the generator\n",
    "    batch_z_to_disply = np.concatenate([latent_points, labels], 1)\n",
    "    X = trained_generator_mnist(batch_z_to_disply, training=False)\n",
    "    fake_values = X.numpy().reshape(X.shape[0], -1)\n",
    "    # generated values should be in [0, 1], because used sigmoid activation\n",
    "    assert np.min(fake_values) >= -0.01 and np.max(fake_values) <= 1.01\n",
    "    mean = fake_values.mean(axis=0)\n",
    "    cov = np.cov(fake_values.T) # 784 x 784\n",
    "    stats_fake[i] = [mean, cov]\n",
    "    \n",
    "fid_list = [] # smaller == better\n",
    "for i in range(nClass):\n",
    "    m, c = stats_real[i]\n",
    "    m_fake, c_fake = stats_fake[i]\n",
    "    fid_list.append(frechet_distance(m, c, m_fake, c_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fid_list = [1,1,2,1.0,2,1,0.1,0.01,1,0]\n",
    "mnist_result = pd.DataFrame()\n",
    "mnist_result['label'] = [str(i) for i in range(nClass)] + ['average']\n",
    "mnist_result['Frechet Inception Distance - MNIST'] = list(fid_list) + [np.mean(fid_list)]\n",
    "mnist_result.set_index('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real disbrituion for each label.\n",
    "# Image data: values are transformed to [0, 1],\n",
    "#             same as that of generator outputs.\n",
    "# Dictionary stats_real, example: stats[0] = [mean_0, cov_0]\n",
    "stats_real = get_mean_cov_for_each_label(datasets='fashion_mnist')\n",
    "\n",
    "\n",
    "# Calculate FID for each label.\n",
    "# 1. For each label i,\n",
    "#    use 1000 fake images to calculate the generated distribution.\n",
    "# 2. Then calculate fid for each label using fake & real distribution.\n",
    "nClass = 10\n",
    "fake_num = 6400\n",
    "stats_fake = {}\n",
    "\n",
    "trained_generator_fmnist = model_fmnist.g # fmnist generator\n",
    "latent_points, _ = generate_latent_points(model_fmnist.z_dim, fake_num, 'uniform')\n",
    "\n",
    "for i in range(nClass):\n",
    "    labels = np.full(shape=(fake_num,), fill_value=i, dtype=np.int32)\n",
    "    labels = tf.one_hot(labels, 10).numpy()\n",
    "    # Get input to the generator\n",
    "    batch_z_to_disply = np.concatenate([latent_points, labels], 1)\n",
    "    X = trained_generator_fmnist(batch_z_to_disply, training=False)\n",
    "    fake_values = X.numpy().reshape(X.shape[0], -1)\n",
    "    # generated values should be in [0, 1], because used sigmoid activation\n",
    "    assert np.min(fake_values) >= -0.01 and np.max(fake_values) <= 1.01\n",
    "    mean = fake_values.mean(axis=0)\n",
    "    cov = np.cov(fake_values.T) # 784 x 784\n",
    "    stats_fake[i] = [mean, cov]\n",
    "    \n",
    "fid_list = [] # smaller == better\n",
    "for i in range(nClass):\n",
    "    m, c = stats_real[i]\n",
    "    m_fake, c_fake = stats_fake[i]\n",
    "    fid_list.append(frechet_distance(m, c, m_fake, c_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_result = pd.DataFrame()\n",
    "fmnist_result['label'] = [str(i) for i in range(nClass)] + ['average']\n",
    "fmnist_result['Frechet Inception Distance - Fashion-MNIST'] = list(fid_list) + [np.mean(fid_list)]\n",
    "fmnist_result.set_index('label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
